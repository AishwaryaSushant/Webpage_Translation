{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0dECsGbkFjC6PKv5Ss9EM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmdwltERrBeY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import PIL\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from io import BytesIO\n",
        "from deep_translator import GoogleTranslator\n",
        "import webbrowser\n",
        "import os\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Path to Tesseract OCR executable\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\PATH\\Tesseract-OCR\\tesseract.exe'\n",
        "\n",
        "# Define the website URL\n",
        "web_url = 'https://www.lge.co.kr/'\n",
        "\n",
        "# Initialize translator\n",
        "translator = GoogleTranslator(source='ko', target='en')\n",
        "\n",
        "# Extract text from an image URL using Tesseract OCR\n",
        "def extract_text_from_image_url(image_url):\n",
        "    try:\n",
        "        response = requests.get(image_url)\n",
        "        response.raise_for_status() # Check for request success\n",
        "        image_data = BytesIO(response.content)\n",
        "        image = Image.open(image_data)\n",
        "        extracted_text = pytesseract.image_to_string(image, lang='eng')\n",
        "        return extracted_text.strip()\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(\"Text extraction from image URL failed:\", req_err)\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(\"Text extraction from image URL failed:\", e)\n",
        "        return \"\"\n",
        "    except PIL.UnidentifiedImageError as img_err:\n",
        "        print(\"Text extraction from image URL failed: Unsupported image format\")\n",
        "        return \"\"\n",
        "\n",
        "# Translate text using Google Translator\n",
        "def translate_text(text):\n",
        "    try:\n",
        "        if text:\n",
        "            translated_text = translator.translate(text)\n",
        "            return translated_text\n",
        "        else:\n",
        "            return \"\"\n",
        "    except Exception as e:\n",
        "        print(\"Translation failed:\", e)\n",
        "        return \"\"\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "# Modify HTML content with translated text and CSS\n",
        "def modify_html_with_translated_text_and_css(html_content, translated_texts, css_files):\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Add link to CSS files\n",
        "        for css_file in css_files:\n",
        "            css_link = soup.new_tag(\"link\", rel=\"stylesheet\", href=css_file)\n",
        "            soup.head.append(css_link)\n",
        "\n",
        "        # Iterate through img tags and replace alt text with translated text\n",
        "        img_tags = soup.find_all('img')\n",
        "        for img_tag in img_tags:\n",
        "            if 'alt' in img_tag.attrs:\n",
        "                img_tag['alt'] = translated_texts.get(img_tag['alt'], img_tag['alt'])  # Use the translated text from the dictionary if available\n",
        "\n",
        "        # Replace text in all elements with translated text\n",
        "        for element in soup.find_all(string=True):\n",
        "            if element.parent and element.parent.name not in ['script', 'style']:\n",
        "                translated_text = translated_texts.get(element, element)\n",
        "                element.replace_with(translated_text)\n",
        "\n",
        "        modified_html = soup.prettify()\n",
        "        return modified_html\n",
        "    except Exception as e:\n",
        "        print(\"Modifying HTML content failed:\", e)\n",
        "        return None\n",
        "\n",
        "# Download and save files from URLs\n",
        "def download_and_save_files(files):\n",
        "    try:\n",
        "        for url in files:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            file_name = os.path.basename(url)\n",
        "            # Remove invalid characters from the filename using regex\n",
        "            file_name = re.sub(r'[\\/:*?\"<>|]', '_', file_name)\n",
        "            with open(file_name, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Downloaded and saved: {file_name}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(\"File download failed:\", req_err)\n",
        "    except Exception as e:\n",
        "        print(\"File download failed:\", e)\n",
        "\n",
        "# Extract CSS and script files from the website\n",
        "def extract_css_and_script_files(web_url):\n",
        "    try:\n",
        "        response = requests.get(web_url)\n",
        "        response.raise_for_status()\n",
        "        html_content = response.text\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        css_files = []\n",
        "        script_files = []\n",
        "\n",
        "        # Extract CSS files\n",
        "        css_tags = soup.find_all('link', rel='stylesheet')\n",
        "        for css_tag in css_tags:\n",
        "            css_url = css_tag.get('href')\n",
        "            if css_url and not css_url.startswith(('http:', 'https:')):\n",
        "                css_url = web_url + css_url if not css_url.startswith('/') else web_url + '/' + css_url\n",
        "                css_files.append(css_url)\n",
        "\n",
        "        # Extract script files\n",
        "        script_tags = soup.find_all('script', src=True)\n",
        "        for script_tag in script_tags:\n",
        "            script_url = script_tag.get('src')\n",
        "            if script_url and not script_url.startswith(('http:', 'https:')):\n",
        "                script_url = web_url + script_url if not script_url.startswith('/') else web_url + '/' + script_url\n",
        "                script_files.append(script_url)\n",
        "\n",
        "        return css_files, script_files\n",
        "\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(\"Request error while extracting files:\", req_err)\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred while extracting files:\", e)\n",
        "        return [], []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Extract CSS and script files\n",
        "        css_files, script_files = extract_css_and_script_files(web_url)\n",
        "\n",
        "        # Download and save script and CSS files\n",
        "        download_and_save_files(script_files)\n",
        "        download_and_save_files(css_files)\n",
        "\n",
        "        # Start processing the web page\n",
        "        response = requests.get(web_url)\n",
        "        response.raise_for_status()\n",
        "        html_content = response.text\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Initialize a dictionary to store translated texts for each element\n",
        "        translated_texts = {}\n",
        "\n",
        "        for element in soup.find_all(string=True):\n",
        "            if element.parent and element.parent.name not in ['script', 'style']:\n",
        "                translated_text = translate_text(element)\n",
        "                translated_texts[element] = translated_text  # Store the translated text\n",
        "\n",
        "        # Iterate through img tags and process text\n",
        "        img_tags = soup.find_all('img')\n",
        "        for img_tag in img_tags:\n",
        "            image_url = img_tag.get('src')\n",
        "            if image_url and not image_url.startswith(('data:', 'http:', 'https:')):\n",
        "                image_url = web_url + image_url if not image_url.startswith('/') else web_url + '/' + image_url\n",
        "                extracted_text = extract_text_from_image_url(image_url)\n",
        "                if extracted_text:\n",
        "                    translated_text = translate_text(extracted_text)\n",
        "                    if translated_text:\n",
        "                        img_tag['alt'] = translated_text\n",
        "\n",
        "            # Store the link address of the img tag in the image_url variable\n",
        "            image_url = img_tag.get('src')\n",
        "            if image_url:\n",
        "                print(\"Image URL:\", image_url)\n",
        "\n",
        "        # Modify HTML with translated text and CSS\n",
        "        modified_html = modify_html_with_translated_text_and_css(str(soup), translated_texts, css_files)\n",
        "\n",
        "        # Save modified HTML and open in browser\n",
        "        if modified_html:\n",
        "            modified_file_path = 'modifiedO_page.html'\n",
        "            with open(modified_file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(modified_html)\n",
        "            webbrowser.open(modified_file_path, new=2)\n",
        "            print(\"Web page modified and opened in browser.\")\n",
        "        else:\n",
        "            print(\"Failed to modify the HTML content.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(\"Request error:\", req_err)\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBT-7yS7rJnb",
        "outputId": "c7c72793-8683-4739-d0f2-3be2bf78e2a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep_translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mYBSrgdrTCH",
        "outputId": "5c56f995-0b16-4ddb-a2d9-bd8c87ca9a38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2023.7.22)\n",
            "Installing collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0gWlNDSsclg",
        "outputId": "344cad9e-640b-46da-9ee4-fba0c029b32b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Selenium\n",
            "  Downloading selenium-4.13.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from Selenium) (2.0.6)\n",
            "Collecting trio~=0.17 (from Selenium)\n",
            "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from Selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from Selenium) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->Selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->Selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->Selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->Selenium)\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->Selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->Selenium) (1.1.3)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->Selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->Selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->Selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, Selenium\n",
            "Successfully installed Selenium-4.13.0 h11-0.14.0 outcome-1.2.0 trio-0.22.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSD35O7Wsl5D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}